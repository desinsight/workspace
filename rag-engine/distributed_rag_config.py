#!/usr/bin/env python3
"""
Desinsight ë¶„ì‚°í˜• RAG ì‹œìŠ¤í…œ - í™˜ê²½ë³„ ì„¤ì • ê´€ë¦¬ì
"""

import os
import platform
import subprocess
import psutil
from dataclasses import dataclass
from typing import Dict, List, Optional
import json

@dataclass
class SystemSpec:
    """ì‹œìŠ¤í…œ ì‚¬ì–‘"""
    model_name: str
    cpu_cores: int
    memory_gb: float
    gpu_info: str
    role: str  # 'embedding', 'inference', 'hybrid'

class DistributedRAGConfig:
    def __init__(self):
        self.system_spec = self.detect_system()
        self.config = self.generate_config()
        
    def detect_system(self) -> SystemSpec:
        """í˜„ì¬ ì‹œìŠ¤í…œ ì‚¬ì–‘ ê°ì§€"""
        print("ğŸ” ì‹œìŠ¤í…œ í™˜ê²½ ê°ì§€ ì¤‘...")
        
        # CPU ì •ë³´
        cpu_cores = psutil.cpu_count()
        memory_gb = psutil.virtual_memory().total / (1024**3)
        
        # Mac ëª¨ë¸ ê°ì§€
        model_name = "Unknown"
        if platform.system() == "Darwin":
            try:
                result = subprocess.run(['system_profiler', 'SPHardwareDataType'], 
                                      capture_output=True, text=True)
                for line in result.stdout.split('\n'):
                    if 'Model Name:' in line:
                        model_name = line.split(':')[1].strip()
                        break
                    elif 'Model Identifier:' in line:
                        model_name = line.split(':')[1].strip()
                        break
            except:
                pass
        
        # GPU ì •ë³´ (ê°„ë‹¨í•˜ê²Œ)
        gpu_info = "Integrated"
        if "M4 Max" in model_name or "Mac Studio" in model_name:
            gpu_info = "M4 Max GPU"
        elif "M2" in model_name or "Mac Mini" in model_name:
            gpu_info = "M2 GPU"
        elif "iMac" in model_name:
            gpu_info = "Intel Integrated"
            
        # ì—­í•  ê²°ì •
        role = self.determine_role(model_name, memory_gb, cpu_cores)
        
        spec = SystemSpec(
            model_name=model_name,
            cpu_cores=cpu_cores,
            memory_gb=memory_gb,
            gpu_info=gpu_info,
            role=role
        )
        
        print(f"âœ… ì‹œìŠ¤í…œ ê°ì§€ ì™„ë£Œ:")
        print(f"  â€¢ ëª¨ë¸: {spec.model_name}")
        print(f"  â€¢ CPU: {spec.cpu_cores}ì½”ì–´")
        print(f"  â€¢ ë©”ëª¨ë¦¬: {spec.memory_gb:.1f}GB")
        print(f"  â€¢ GPU: {spec.gpu_info}")
        print(f"  â€¢ ì—­í• : {spec.role}")
        
        return spec
    
    def determine_role(self, model_name: str, memory_gb: float, cpu_cores: int) -> str:
        """í•˜ë“œì›¨ì–´ ê¸°ë°˜ ì—­í•  ê²°ì •"""
        if "Mac Studio" in model_name and "M4 Max" in model_name:
            return "inference"  # ê³ ì„±ëŠ¥ ì¶”ë¡  ì„œë²„
        elif "Mac Mini" in model_name and "M2" in model_name:
            return "embedding"  # ì„ë² ë”© ìƒì„± ì„œë²„
        elif memory_gb >= 32:
            return "hybrid"     # ë³µí•© ì²˜ë¦¬ ê°€ëŠ¥
        else:
            return "embedding"  # ê¸°ë³¸ì ìœ¼ë¡œ ì„ë² ë”© ì²˜ë¦¬
    
    def generate_config(self) -> Dict:
        """ì—­í• ë³„ ìµœì í™” ì„¤ì • ìƒì„±"""
        base_config = {
            'system': {
                'role': self.system_spec.role,
                'model_name': self.system_spec.model_name,
                'cpu_cores': self.system_spec.cpu_cores,
                'memory_gb': self.system_spec.memory_gb
            },
            'ollama': {},
            'rag': {},
            'network': {},
            'storage': {}
        }
        
        if self.system_spec.role == "inference":
            # ì‚¬ë¬´ì‹¤ Mac Studio M4 Max - ê³ ì„±ëŠ¥ ì¶”ë¡  ìµœì í™”
            base_config.update({
                'ollama': {
                    'models': ['llama3.1:70b', 'codellama:34b', 'mistral:7b'],
                    'concurrent_requests': 4,
                    'context_length': 8192,
                    'gpu_layers': -1  # ëª¨ë“  ë ˆì´ì–´ GPU ì‚¬ìš©
                },
                'rag': {
                    'search_k': 5,
                    'max_context_length': 4000,
                    'temperature': 0.3,
                    'enable_caching': True
                },
                'network': {
                    'listen_port': 8000,
                    'enable_api': True,
                    'cors_origins': ['*']
                }
            })
            
        elif self.system_spec.role == "embedding":
            # ì§‘ Mac Mini M2 Pro - ì„ë² ë”© ìƒì„± ìµœì í™”
            base_config.update({
                'ollama': {
                    'models': ['llama3.2:3b', 'nomic-embed-text'],
                    'concurrent_requests': 2,
                    'context_length': 4096
                },
                'rag': {
                    'embedding_batch_size': 32,
                    'vector_dimensions': 384,
                    'similarity_threshold': 0.7,
                    'enable_preprocessing': True
                },
                'storage': {
                    'vector_db_path': './vector_store',
                    'backup_enabled': True,
                    'sync_to_cloud': True
                }
            })
            
        else:  # hybrid
            # ê· í˜•ì¡íŒ ì„¤ì •
            base_config.update({
                'ollama': {
                    'models': ['llama3.2:3b', 'llama3.1:8b'],
                    'concurrent_requests': 2,
                    'context_length': 4096
                },
                'rag': {
                    'search_k': 3,
                    'max_context_length': 2000,
                    'temperature': 0.5
                }
            })
            
        return base_config
    
    def save_config(self, filename: str = "rag_config.json"):
        """ì„¤ì • íŒŒì¼ ì €ì¥"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.config, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ ì„¤ì • ì €ì¥ë¨: {filename}")
    
    def print_deployment_guide(self):
        """ë°°í¬ ê°€ì´ë“œ ì¶œë ¥"""
        print("\nğŸš€ ë¶„ì‚° RAG ì‹œìŠ¤í…œ ë°°í¬ ê°€ì´ë“œ")
        print("="*50)
        
        if self.system_spec.role == "inference":
            print("ğŸ“ í˜„ì¬ í™˜ê²½: ì¶”ë¡  ì„œë²„ (ì‚¬ë¬´ì‹¤)")
            print("\nğŸ¯ ê¶Œì¥ ì‘ì—…:")
            print("1. ëŒ€í˜• LLM ëª¨ë¸ ì„¤ì¹˜:")
            print("   ollama pull llama3.1:70b")
            print("   ollama pull codellama:34b")
            print("\n2. API ì„œë²„ ì‹œì‘:")
            print("   python3 inference_server.py")
            print("\n3. ë„¤íŠ¸ì›Œí¬ ì„¤ì •:")
            print("   - í¬íŠ¸ 8000 ê°œë°©")
            print("   - ì§‘ í™˜ê²½ì—ì„œ ì ‘ê·¼ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì •")
            
        elif self.system_spec.role == "embedding":
            print("ğŸ“ í˜„ì¬ í™˜ê²½: ì„ë² ë”© ì„œë²„ (ì§‘)")
            print("\nğŸ¯ ê¶Œì¥ ì‘ì—…:")
            print("1. ì„ë² ë”© ëª¨ë¸ ì„¤ì¹˜:")
            print("   ollama pull nomic-embed-text")
            print("\n2. ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘:")
            print("   python3 nas_data_collector.py")
            print("\n3. ë²¡í„° DB êµ¬ì¶•:")
            print("   python3 build_vector_store.py")
            
        else:  # hybrid
            print("ğŸ“ í˜„ì¬ í™˜ê²½: í•˜ì´ë¸Œë¦¬ë“œ (ê°œë°œ/í…ŒìŠ¤íŠ¸)")
            print("\nğŸ¯ ê¶Œì¥ ì‘ì—…:")
            print("1. ê¸°ë³¸ ëª¨ë¸ í™•ì¸:")
            print("   ollama list")
            print("\n2. ë¡œì»¬ í…ŒìŠ¤íŠ¸:")
            print("   python3 simple_rag_basic.py")
        
        print(f"\nğŸ’¡ í˜„ì¬ ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤:")
        print(f"   â€¢ CPU í™œìš©ë„: {psutil.cpu_percent()}%")
        print(f"   â€¢ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {psutil.virtual_memory().percent}%")
        print(f"   â€¢ ë””ìŠ¤í¬ ì—¬ìœ  ê³µê°„: {psutil.disk_usage('/').free / (1024**3):.1f}GB")

def main():
    """ì„¤ì • ê´€ë¦¬ì ì‹¤í–‰"""
    print("ğŸ¯ Desinsight ë¶„ì‚°í˜• RAG ì‹œìŠ¤í…œ ì„¤ì • ê´€ë¦¬ì\n")
    
    config_manager = DistributedRAGConfig()
    config_manager.save_config()
    config_manager.print_deployment_guide()
    
    print("\nğŸ”¥ ë‹¤ìŒ ë‹¨ê³„:")
    print("1. NAS ë°ì´í„° ìˆ˜ì§‘ê¸° ì‹¤í–‰")
    print("2. í™˜ê²½ë³„ ìµœì í™” ì„¤ì • ì ìš©") 
    print("3. ë¶„ì‚° ë„¤íŠ¸ì›Œí¬ ì—°ê²° í…ŒìŠ¤íŠ¸")

if __name__ == "__main__":
    main() 