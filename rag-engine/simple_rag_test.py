#!/usr/bin/env python3
"""
Desinsight ë¶„ì‚°í˜• RAG ì‹œìŠ¤í…œ - ê¸°ë³¸ í…ŒìŠ¤íŠ¸
"""

import os
import ollama
from sentence_transformers import SentenceTransformer
import numpy as np
from typing import List, Dict
import json
from datetime import datetime

class SimpleRAGSystem:
    def __init__(self):
        print("ğŸ¯ Desinsight RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”...")
        
        # Ollama ì—°ê²°
        self.ollama_client = ollama.Client()
        
        # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (ê°€ë²¼ìš´ ë²„ì „)
        print("ğŸ“¦ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        
        # ë©”ëª¨ë¦¬ ë²¡í„° ì €ì¥ì†Œ (ì„ì‹œ)
        self.vector_store = []
        self.documents = []
        
        print("âœ… RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")
    
    def add_document(self, text: str, metadata: Dict = None):
        """ë¬¸ì„œë¥¼ ë²¡í„° ì €ì¥ì†Œì— ì¶”ê°€"""
        print(f"ğŸ“„ ë¬¸ì„œ ì¶”ê°€: {text[:50]}...")
        
        # í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
        embedding = self.embedding_model.encode(text)
        
        # ë©”íƒ€ë°ì´í„° ì„¤ì •
        if metadata is None:
            metadata = {}
        metadata['added_at'] = datetime.now().isoformat()
        
        # ì €ì¥
        doc_entry = {
            'text': text,
            'embedding': embedding,
            'metadata': metadata
        }
        
        self.documents.append(doc_entry)
        self.vector_store.append(embedding)
        
        print(f"âœ… ë¬¸ì„œ ì¶”ê°€ ì™„ë£Œ (ì´ {len(self.documents)}ê°œ)")
    
    def similarity_search(self, query: str, k: int = 3) -> List[Dict]:
        """ìœ ì‚¬ë„ ê²€ìƒ‰"""
        if not self.documents:
            return []
        
        print(f"ğŸ” ê²€ìƒ‰ ì¿¼ë¦¬: {query}")
        
        # ì¿¼ë¦¬ ì„ë² ë”©
        query_embedding = self.embedding_model.encode(query)
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = []
        for i, doc_embedding in enumerate(self.vector_store):
            similarity = np.dot(query_embedding, doc_embedding) / (
                np.linalg.norm(query_embedding) * np.linalg.norm(doc_embedding)
            )
            similarities.append((similarity, i))
        
        # ìƒìœ„ kê°œ ë¬¸ì„œ ë°˜í™˜
        similarities.sort(reverse=True)
        results = []
        
        for sim_score, doc_idx in similarities[:k]:
            doc = self.documents[doc_idx]
            results.append({
                'text': doc['text'],
                'similarity': float(sim_score),
                'metadata': doc['metadata']
            })
            print(f"  ğŸ“Œ ìœ ì‚¬ë„ {sim_score:.3f}: {doc['text'][:50]}...")
        
        return results
    
    def ask_question(self, question: str) -> str:
        """RAG ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ"""
        print(f"\nâ“ ì§ˆë¬¸: {question}")
        
        # 1. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        relevant_docs = self.similarity_search(question, k=3)
        
        if not relevant_docs:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # 2. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = "\n".join([doc['text'] for doc in relevant_docs])
        
        # 3. í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""
ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:

ì •ë³´:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€:"""
        
        print("ğŸ¤– LLM ë‹µë³€ ìƒì„± ì¤‘...")
        
        # 4. Ollamaë¡œ ë‹µë³€ ìƒì„±
        try:
            response = self.ollama_client.chat(
                model='llama3.2:3b',
                messages=[
                    {
                        'role': 'system',
                        'content': 'ë‹¹ì‹ ì€ ê±´ì¶• ë° ê±´ì„¤ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ í•´ì£¼ì„¸ìš”.'
                    },
                    {
                        'role': 'user',
                        'content': prompt
                    }
                ]
            )
            
            answer = response['message']['content']
            print(f"ğŸ’¡ ë‹µë³€: {answer}")
            return answer
            
        except Exception as e:
            print(f"âŒ LLM ì˜¤ë¥˜: {e}")
            return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

def main():
    """RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    print("ğŸš€ Desinsight ë¶„ì‚°í˜• RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹œì‘\n")
    
    # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    rag = SimpleRAGSystem()
    
    # ìƒ˜í”Œ ê±´ì¶• ë¬¸ì„œë“¤ ì¶”ê°€
    print("\nğŸ“š ìƒ˜í”Œ ë¬¸ì„œ ì¶”ê°€ ì¤‘...")
    
    documents = [
        {
            'text': 'ì•„íŒŒíŠ¸ ê±´ì„¤ì—ì„œ ì² ê·¼ì½˜í¬ë¦¬íŠ¸ êµ¬ì¡°ëŠ” ë‚´êµ¬ì„±ê³¼ ê²½ì œì„±ì„ ë™ì‹œì— í™•ë³´í•  ìˆ˜ ìˆëŠ” ìµœì ì˜ êµ¬ì¡° ë°©ì‹ì…ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ 25~30ì¸µ ê±´ë¬¼ì— ì í•©í•˜ë©°, ì••ì¶•ê°•ë„ 24~30MPaì˜ ì½˜í¬ë¦¬íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.',
            'metadata': {'type': 'structural', 'category': 'concrete'}
        },
        {
            'text': 'ê±´ì¶•ë¬¼ì˜ ë‹¨ì—´ì„±ëŠ¥ì€ ì™¸ë²½ ë‹¨ì—´ì¬ì˜ ë‘ê»˜ì™€ ì§ê²°ë©ë‹ˆë‹¤. ì¤‘ë¶€ì§€ì—­ ê¸°ì¤€ìœ¼ë¡œ ì™¸ë²½ ë‹¨ì—´ì¬ëŠ” ìµœì†Œ 100mm ì´ìƒ, ì§€ë¶•ì€ 200mm ì´ìƒ ì‹œê³µí•´ì•¼ ì—ë„ˆì§€ íš¨ìœ¨ 1ë“±ê¸‰ì„ ë‹¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.',
            'metadata': {'type': 'insulation', 'category': 'energy_efficiency'}
        },
        {
            'text': 'ê±´ì„¤í˜„ì¥ì—ì„œ ì•ˆì „ê´€ë¦¬ëŠ” ìµœìš°ì„  ê³¼ì œì…ë‹ˆë‹¤. í—¬ë©§, ì•ˆì „ë²¨íŠ¸, ì•ˆì „í™” ì°©ìš©ì€ ê¸°ë³¸ì´ë©°, ê³ ì†Œì‘ì—… ì‹œì—ëŠ” ë°˜ë“œì‹œ ì•ˆì „ë‚œê°„ê³¼ ì¶”ë½ë°©ì§€ë§ì„ ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.',
            'metadata': {'type': 'safety', 'category': 'construction_safety'}
        },
        {
            'text': 'ê±´ì¶• ë„ë©´ì—ì„œ 1:100 ì¶•ì²™ì€ í‰ë©´ë„ì™€ ì…ë©´ë„ ì‘ì„±ì— ê°€ì¥ ë§ì´ ì‚¬ìš©ë©ë‹ˆë‹¤. ì´ ì¶•ì²™ì—ì„œëŠ” ë²½ì²´ ë‘ê»˜, ë¬¸ì°½í˜¸ ìœ„ì¹˜, êµ¬ì¡°ì²´ ë°°ì¹˜ê°€ ëª…í™•í•˜ê²Œ í‘œí˜„ë˜ì–´ ì‹œê³µìê°€ ì´í•´í•˜ê¸° ì‰½ìŠµë‹ˆë‹¤.',
            'metadata': {'type': 'drawing', 'category': 'architectural_plan'}
        }
    ]
    
    for doc in documents:
        rag.add_document(doc['text'], doc['metadata'])
    
    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤
    print("\nğŸ§ª RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤:")
    
    test_questions = [
        "ì•„íŒŒíŠ¸ ê±´ì„¤ì— ì í•©í•œ êµ¬ì¡°ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
        "ê±´ë¬¼ ë‹¨ì—´ì¬ ë‘ê»˜ëŠ” ì–´ë–»ê²Œ ì •í•´ì•¼ í•˜ë‚˜ìš”?",
        "ê±´ì„¤í˜„ì¥ ì•ˆì „ê´€ë¦¬ ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”.",
        "ê±´ì¶•ë„ë©´ ì¶•ì²™ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”."
    ]
    
    for i, question in enumerate(test_questions, 1):
        print(f"\n{'='*60}")
        print(f"ğŸ”¸ í…ŒìŠ¤íŠ¸ {i}/4")
        answer = rag.ask_question(question)
        print(f"{'='*60}")
    
    print("\nğŸ¯ RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("\nğŸ“Š ê²°ê³¼ ìš”ì•½:")
    print(f"  â€¢ ë¬¸ì„œ ìˆ˜: {len(rag.documents)}ê°œ")
    print(f"  â€¢ ì„ë² ë”© ëª¨ë¸: {rag.embedding_model.model_name}")
    print(f"  â€¢ LLM ëª¨ë¸: llama3.2:3b")
    print(f"  â€¢ ë²¡í„° ì°¨ì›: {len(rag.vector_store[0]) if rag.vector_store else 0}")

if __name__ == "__main__":
    main() 